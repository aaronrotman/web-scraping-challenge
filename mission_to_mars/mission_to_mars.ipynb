{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome driver setup\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "\n",
    "# Dictionary to store the scraped data\n",
    "mars_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Mars News\n",
    "\n",
    "URL: https://mars.nasa.gov/news/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the URL for the NASA Mars News website\n",
    "news_url = \"https://mars.nasa.gov/news/\"\n",
    "\n",
    "# Instantiate Browser\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Attempt to scrape the NASA Mars News website\n",
    "try:\n",
    "    # Visit the url\n",
    "    browser.visit(news_url)\n",
    "    # Wait for the website to fully load\n",
    "    time.sleep(5)\n",
    "    # Scrape the html from the site\n",
    "    html = browser.html\n",
    "    # Close the browser\n",
    "    browser.quit()\n",
    "\n",
    "    # Create Beautiful Soup object from scraped html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # Extract and store the latest news title and paragraph description\n",
    "    latest_news = soup.find('div', class_=\"image_and_description_container\")\n",
    "    news_title = latest_news.find('div', class_='content_title').text\n",
    "    news_p = latest_news.find('div', class_='article_teaser_body').text\n",
    "\n",
    "    # Add the news title and paragraph description to the scraping results dictionary: 'mars_dict'\n",
    "    mars_dict['latest_news'] = {'title': news_title, 'paragraph': news_p}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Handle errors\n",
    "except Exception as e:\n",
    "    # Print exception\n",
    "    print(e)\n",
    "    mars_dict['latest_news'] = {'title': 'Scraping Failed', 'paragrah': 'Scraping Failed'}\n",
    "    # Close the browser\n",
    "    browser.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Featured Image\n",
    "URL: https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL\n",
    "feat_img_base_url = \"https://www.jpl.nasa.gov/\"\n",
    "# URL to scrape\n",
    "jpl_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "\n",
    "# Instantiate Browser\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Attempt to scrape the JPL Mars website's featured image\n",
    "try: \n",
    "    \n",
    "    # Visit the url\n",
    "    browser.visit(jpl_url)\n",
    "    # Scrape the html from the site\n",
    "    html = browser.html\n",
    "    \n",
    "    # Create Beautiful Soup object from scraped html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    # Extract the URL for the featured image page\n",
    "    feat_image = soup.find('div', class_=\"carousel_items\")\n",
    "    feat_img_url = feat_image.a['data-link']\n",
    "\n",
    "    # Navigate to the article page for the featured image\n",
    "    browser.visit(f\"{feat_img_base_url}{feat_img_url}\")\n",
    "    \n",
    "    # Scrape the html from the featured image page\n",
    "    feat_img_html = browser.html\n",
    "    # Create Beautiful Soup object from scraped html\n",
    "    feat_soup = bs(feat_img_html, \"html.parser\")\n",
    "\n",
    "    # Extract the URL for the full size featured image\n",
    "    img_fig = feat_soup.find('figure', class_='lede')\n",
    "    img_fig_url = img_fig.a['href']\n",
    "    \n",
    "    # Add the full size featured image url to the scraping results dictionary: 'mars_dict'\n",
    "    mars_dict['feat_image_url'] = f\"{feat_img_base_url}{img_fig_url}\"\n",
    "    \n",
    "    # Close the browser\n",
    "    browser.quit()\n",
    "# Handle errors\n",
    "except Exception as e:\n",
    "    # Print exception\n",
    "    print(e)\n",
    "    # Close the browser\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Weather\n",
    "URL: https://twitter.com/marswxreport?lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to scrape\n",
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "# Instantiate Browser\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Attempt to scrape the Mars weather twitter account\n",
    "try: \n",
    "\n",
    "    browser.visit(url)\n",
    "    # Wait for the website to load\n",
    "    time.sleep(5)\n",
    "    # Create Beautiful Soup object from scraped html\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    weather_data = soup.find(\"div\", class_=\"css-901oao r-hkyrab r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")\n",
    "    # Replace line breaks with spaces\n",
    "    replaced_weather_data = weather_data.text.replace(\"\\n\", \" \")\n",
    "    mars_dict['weather'] = replaced_weather_data\n",
    "    browser.quit()\n",
    "# Handle errors\n",
    "except Exception as e:\n",
    "    # Print exception\n",
    "    print(e)\n",
    "    browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InSight sol 543 (2020-06-06) low -92.9ºC (-135.3ºF) high -6.6ºC (20.2ºF) winds from the WNW at 7.2 m/s (16.0 mph) gusting to 21.0 m/s (47.0 mph) pressure at 7.40 hPa'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data = soup.find(\"div\", class_=\"css-901oao r-hkyrab r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")\n",
    "weather_data.text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts\n",
    "URL: https://space-facts.com/mars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the URL to scrape\n",
    "url = 'https://space-facts.com/mars'\n",
    "\n",
    "# Attempt to scrape the Mars facts website\n",
    "try: \n",
    "    # Scrape tabular data from the website using pandas\n",
    "    tables = pd.read_html(url)\n",
    "\n",
    "    # Store the Mars fact table\n",
    "    fact_table = tables[0]\n",
    "\n",
    "    # Clean up table dataframe\n",
    "    # Rename columns\n",
    "    fact_table.columns = ['Attribute', 'Value']\n",
    "    # Set 'Attribute' column as the index\n",
    "    fact_table.set_index('Attribute', inplace=True)\n",
    "\n",
    "    # Convert the fact table to an html string\n",
    "    html_table = fact_table.to_html()\n",
    "    \n",
    "    # Add the Mars fact table html string to the scraping results dictionary: 'mars_dict'\n",
    "    mars_dict['fact_table'] = html_table\n",
    "    \n",
    "# Handle errors\n",
    "except Exception as e:\n",
    "    # Print exception\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres\n",
    "URL: https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL\n",
    "hemi_base_url = 'https://astrogeology.usgs.gov'\n",
    "# URL to scrape\n",
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "# Instantiate Browser\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Go to the mars hemispheres website\n",
    "browser.visit(url)\n",
    "# Scrape the html\n",
    "html = browser.html\n",
    "# Create Beautiful Soup object from scraped html\n",
    "soup = bs(html, 'lxml')\n",
    "\n",
    "# Extract the div containing the link to each hemisphere page\n",
    "image_links = soup.find_all('div', class_='description')\n",
    "\n",
    "# List to store full size hemisphere image urls\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Attempt to scrape each hemisphere page to extract the full size image URL\n",
    "for link in image_links:\n",
    "    # Visit hemisphere page\n",
    "    try:\n",
    "        print(f'Visiting: {link.h3.text}')\n",
    "        # Wait for website to load\n",
    "        time.sleep(1)\n",
    "        print('Visiting...')\n",
    "        # Visit hemisphere page\n",
    "        browser.click_link_by_partial_text(link.h3.text)\n",
    "        # Wait for website to load\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Scrape html\n",
    "        html = browser.html\n",
    "        # Create Beautiful Soup object from scraped html\n",
    "        soup = bs(html, 'lxml')\n",
    "\n",
    "        # Store the image title\n",
    "        title = soup.find('h2', class_='title')\n",
    "        # Store the image URL\n",
    "        image = soup.find('img', class_='wide-image')\n",
    "        \n",
    "        # Add the image title and url to the list of hemisphere image urls\n",
    "        hemisphere_image_urls.append({'title': title.text, 'img_url': f\"{hemi_base_url}{image['src']}\"})\n",
    "        print('Data scraped')\n",
    "        \n",
    "        # Return to the previous page\n",
    "        browser.visit(url)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        browser.visit(url)\n",
    "\n",
    "# Close the browser\n",
    "browser.quit()\n",
    "# Add the Mars hemisphere image URLs to the scraping results dictionary: 'mars_dict'\n",
    "mars_dict['hemisphere_image_urls'] = hemisphere_image_urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
