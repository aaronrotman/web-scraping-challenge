{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome driver setup\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "\n",
    "# Dictionary to store the scraped data\n",
    "mars_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Mars News\n",
    "\n",
    "URL: https://mars.nasa.gov/news/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the URL for the NASA Mars News website\n",
    "news_url = \"https://mars.nasa.gov/news/\"\n",
    "\n",
    "# Instantiate Browser\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Attempt to scrape the NASA Mars News website\n",
    "try:\n",
    "    # Visit the url\n",
    "    browser.visit(news_url)\n",
    "    # Wait for the website to fully load\n",
    "    time.sleep(5)\n",
    "    # Scrape the html from the site\n",
    "    html = browser.html\n",
    "    # Close the browser\n",
    "    browser.quit()\n",
    "\n",
    "    # Create Beautiful Soup object from scraped html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # Extract and store the latest news title and paragraph description\n",
    "    latest_news = soup.find('div', class_=\"image_and_description_container\")\n",
    "    news_title = latest_news.find('div', class_='content_title').text\n",
    "    news_p = latest_news.find('div', class_='article_teaser_body').text\n",
    "\n",
    "    # Add the news title and paragraph description to the scraping results dictionary: 'mars_dict'\n",
    "    mars_dict['latest_news'] = {'title': news_title, 'paragraph': news_p}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Handle errors\n",
    "except Exception as e:\n",
    "    # Print exception\n",
    "    print(e)\n",
    "    mars_dict['latest_news'] = {'title': 'Scraping Failed', 'paragrah': 'Scraping Failed'}\n",
    "    # Close the browser\n",
    "    browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\n",
      "The Extraordinary Sample-Gathering System of NASA's Perseverance Mars Rover\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Paragraph:\n",
      "Two astronauts collected Moon rocks on Apollo 11. It will take three robotic systems working together to gather up the first Mars rock samples for return to Earth.\n"
     ]
    }
   ],
   "source": [
    "# Print the title and paragraph\n",
    "print(f\"Title:\\n{mars_dict['latest_news']['title']}\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Paragraph:\\n{mars_dict['latest_news']['paragraph']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Featured Image\n",
    "URL: https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL\n",
    "feat_img_base_url = \"https://www.jpl.nasa.gov/\"\n",
    "# URL to scrape\n",
    "jpl_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "\n",
    "# Instantiate Browser\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Attempt to scrape the JPL Mars website's featured image\n",
    "try: \n",
    "    \n",
    "    # Visit the url\n",
    "    browser.visit(jpl_url)\n",
    "    # Scrape the html from the site\n",
    "    html = browser.html\n",
    "    \n",
    "    # Create Beautiful Soup object from scraped html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    # Extract the URL for the featured image page\n",
    "    feat_image = soup.find('div', class_=\"carousel_items\")\n",
    "    feat_img_url = feat_image.a['data-link']\n",
    "\n",
    "    # Navigate to the article page for the featured image\n",
    "    browser.visit(f\"{feat_img_base_url}{feat_img_url}\")\n",
    "    \n",
    "    # Scrape the html from the featured image page\n",
    "    feat_img_html = browser.html\n",
    "    # Create Beautiful Soup object from scraped html\n",
    "    feat_soup = bs(feat_img_html, \"html.parser\")\n",
    "\n",
    "    # Extract the URL for the full size featured image\n",
    "    img_fig = feat_soup.find('figure', class_='lede')\n",
    "    img_fig_url = img_fig.a['href']\n",
    "    \n",
    "    # Add the full size featured image url to the scraping results dictionary: 'mars_dict'\n",
    "    mars_dict['feat_image_url'] = f\"{feat_img_base_url}{img_fig_url}\"\n",
    "    \n",
    "    # Close the browser\n",
    "    browser.quit()\n",
    "# Handle errors\n",
    "except Exception as e:\n",
    "    # Print exception\n",
    "    print(e)\n",
    "    # Close the browser\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featured Image URL:\n",
      "https://www.jpl.nasa.gov//spaceimages/images/largesize/PIA17767_hires.jpg\n"
     ]
    }
   ],
   "source": [
    "# Print the feature image url\n",
    "print(f\"Featured Image URL:\\n{mars_dict['feat_image_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Weather\n",
    "URL: https://twitter.com/marswxreport?lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to scrape\n",
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "# Instantiate Browser\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Attempt to scrape the Mars weather twitter account\n",
    "try: \n",
    "\n",
    "    browser.visit(url)\n",
    "    # Wait for the website to load\n",
    "    time.sleep(5)\n",
    "    # Create Beautiful Soup object from scraped html\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n",
    "    weather_data = soup.find(\"div\", class_=\"css-901oao r-hkyrab r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")\n",
    "    # Replace line breaks with spaces\n",
    "    replaced_weather_data = weather_data.text.replace(\"\\n\", \" \")\n",
    "    mars_dict['weather'] = replaced_weather_data\n",
    "    browser.quit()\n",
    "# Handle errors\n",
    "except Exception as e:\n",
    "    # Print exception\n",
    "    print(e)\n",
    "    browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Weather on Mars:\n",
      "InSight sol 543 (2020-06-06) low -92.9ºC (-135.3ºF) high -6.6ºC (20.2ºF) winds from the WNW at 7.2 m/s (16.0 mph) gusting to 21.0 m/s (47.0 mph) pressure at 7.40 hPa\n"
     ]
    }
   ],
   "source": [
    "# Print out the latest weather on Mars\n",
    "print(f\"Current Weather on Mars:\\n{mars_dict['weather']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts\n",
    "URL: https://space-facts.com/mars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the URL to scrape\n",
    "url = 'https://space-facts.com/mars'\n",
    "\n",
    "# Attempt to scrape the Mars facts website\n",
    "try: \n",
    "    # Scrape tabular data from the website using pandas\n",
    "    tables = pd.read_html(url)\n",
    "\n",
    "    # Store the Mars fact table\n",
    "    fact_table = tables[0]\n",
    "\n",
    "    # Clean up table dataframe\n",
    "    # Rename columns\n",
    "    fact_table.columns = ['Attribute', 'Value']\n",
    "    # Set 'Attribute' column as the index\n",
    "    fact_table.set_index('Attribute', inplace=True)\n",
    "\n",
    "    # Convert the fact table to an html string\n",
    "    html_table = fact_table.to_html()\n",
    "    \n",
    "    # Add the Mars fact table html string to the scraping results dictionary: 'mars_dict'\n",
    "    mars_dict['fact_table'] = html_table\n",
    "    \n",
    "# Handle errors\n",
    "except Exception as e:\n",
    "    # Print exception\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>Value</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Attribute</th>\n",
      "      <th></th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>Equatorial Diameter:</th>\n",
      "      <td>6,792 km</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Polar Diameter:</th>\n",
      "      <td>6,752 km</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Mass:</th>\n",
      "      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Moons:</th>\n",
      "      <td>2 (Phobos &amp; Deimos)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Orbit Distance:</th>\n",
      "      <td>227,943,824 km (1.38 AU)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Orbit Period:</th>\n",
      "      <td>687 days (1.9 years)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Surface Temperature:</th>\n",
      "      <td>-87 to -5 °C</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>First Record:</th>\n",
      "      <td>2nd millennium BC</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>Recorded By:</th>\n",
      "      <td>Egyptian astronomers</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "# Print out the Mars fact table html string\n",
    "print(mars_dict['fact_table'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres\n",
    "URL: https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting: Cerberus Hemisphere Enhanced\n",
      "Visiting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py:528: FutureWarning: browser.find_link_by_partial_text is deprecated. Use browser.links.find_by_partial_text instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped\n",
      "Visiting: Schiaparelli Hemisphere Enhanced\n",
      "Visiting...\n",
      "Data scraped\n",
      "Visiting: Syrtis Major Hemisphere Enhanced\n",
      "Visiting...\n",
      "Data scraped\n",
      "Visiting: Valles Marineris Hemisphere Enhanced\n",
      "Visiting...\n",
      "Data scraped\n"
     ]
    }
   ],
   "source": [
    "# Base URL\n",
    "hemi_base_url = 'https://astrogeology.usgs.gov'\n",
    "# URL to scrape\n",
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "# Instantiate Browser\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Go to the mars hemispheres website\n",
    "browser.visit(url)\n",
    "# Scrape the html\n",
    "html = browser.html\n",
    "# Create Beautiful Soup object from scraped html\n",
    "soup = bs(html, 'lxml')\n",
    "\n",
    "# Extract the div containing the link to each hemisphere page\n",
    "image_links = soup.find_all('div', class_='description')\n",
    "\n",
    "# List to store full size hemisphere image urls\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Attempt to scrape each hemisphere page to extract the full size image URL\n",
    "for link in image_links:\n",
    "    # Visit hemisphere page\n",
    "    try:\n",
    "        print(f'Visiting: {link.h3.text}')\n",
    "        # Wait for website to load\n",
    "        time.sleep(1)\n",
    "        print('Visiting...')\n",
    "        # Visit hemisphere page\n",
    "        browser.click_link_by_partial_text(link.h3.text)\n",
    "        # Wait for website to load\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Scrape html\n",
    "        html = browser.html\n",
    "        # Create Beautiful Soup object from scraped html\n",
    "        soup = bs(html, 'lxml')\n",
    "\n",
    "        # Store the image title\n",
    "        title = soup.find('h2', class_='title')\n",
    "        # Store the image URL\n",
    "        image = soup.find('img', class_='wide-image')\n",
    "        \n",
    "        # Add the image title and url to the list of hemisphere image urls\n",
    "        hemisphere_image_urls.append({'title': title.text, 'img_url': f\"{hemi_base_url}{image['src']}\"})\n",
    "        print('Data scraped')\n",
    "        \n",
    "        # Return to the previous page\n",
    "        browser.visit(url)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        browser.visit(url)\n",
    "\n",
    "# Close the browser\n",
    "browser.quit()\n",
    "# Add the Mars hemisphere image URLs to the scraping results dictionary: 'mars_dict'\n",
    "mars_dict['hemisphere_image_urls'] = hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerberus Hemisphere Enhanced:\n",
      "https://astrogeology.usgs.gov/cache/images/f5e372a36edfa389625da6d0cc25d905_cerberus_enhanced.tif_full.jpg\n",
      "Schiaparelli Hemisphere Enhanced:\n",
      "https://astrogeology.usgs.gov/cache/images/3778f7b43bbbc89d6e3cfabb3613ba93_schiaparelli_enhanced.tif_full.jpg\n",
      "Syrtis Major Hemisphere Enhanced:\n",
      "https://astrogeology.usgs.gov/cache/images/555e6403a6ddd7ba16ddb0e471cadcf7_syrtis_major_enhanced.tif_full.jpg\n",
      "Valles Marineris Hemisphere Enhanced:\n",
      "https://astrogeology.usgs.gov/cache/images/b3c7c6c9138f57b4756be9b9c43e3a48_valles_marineris_enhanced.tif_full.jpg\n"
     ]
    }
   ],
   "source": [
    "# Print out the Mars hemisphere image urls\n",
    "for url in hemisphere_image_urls:\n",
    "    print(f\"{url['title']}:\\n{url['img_url']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
